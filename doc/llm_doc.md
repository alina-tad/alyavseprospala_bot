## LLM: primary и fallback-модели

В боте реализована схема работы с несколькими моделями LLM через OpenRouter: одна **primary** модель и один или несколько **fallback**-ов. Это позволяет получать ответы даже если основная модель недоступна или выдает неудовлетворительный результат.

### Зачем это нужно
- **Надёжность**: если у основной модели 4xx/5xx ошибки или отключены эндпоинты, бот автоматически попробует резервные.
- **Качество**: если ответ primary выглядит «слишком сухим/без структуры», включается резервная модель с иной стилистикой/качеством.
- **Экономия**: можно использовать более дешёвую/быструю модель как primary и подключать более «тяжёлую» только при необходимости.

### Как это работает
1. Формируется список сообщений: system-промпт + сообщение пользователя.
2. Запрос отправляется в primary-модель `LLM_PRIMARY_MODEL`.
3. Эвристика качества проверяет текст (наличие структуры «ключевые символы / практический вывод», минимальная длина, отсутствие повторов инструкций). Если всё ок — ответ возвращается.
4. Если primary вернула ошибку или ответ выглядит «сухо», бот последовательно перебирает список `LLM_FALLBACK_MODELS` до первого успешного результата.

### Конфигурация (через .env)
- `LLM_PRIMARY_MODEL=qwen/qwen3-8b`
- `LLM_FALLBACK_MODELS=qwen/qwq-32b:free`  (через запятую, если моделей несколько)
- `LLM_FALLBACK_ENABLED=true`

Старая переменная `LLM_FALLBACK_MODEL` поддерживается для совместимости, но лучше использовать `LLM_FALLBACK_MODELS`.

### Логи и метаданные
- В логах при старте: `LLM primary: <primary>; fallbacks: [..]`.
- В ответах, сохранённых в `data/conversations.json`, у сообщений ассистента добавляется `metadata`:
  - `model`: фактическая модель, давшая ответ
  - `fallback`: `true/false`
  - `fallback_index`: индекс в списке резервов (если применялся)
  - `finish_reason`, `continued`, `continuations`: статус завершения и автодогенерации, если ответ обрезался по длине
  - `usage` (если провайдер вернул токены)

### Типичные случаи срабатывания fallback
- У primary 404/403/5xx по API или «No endpoints found».
- Ответ primary слишком короткий, без структурных маркеров («Ключевые символы», «Практический вывод») или повторяет инструкции системного промпта.
- Нестабильная генерация, когда нужно более «вдумчивое» продолжение.

### Примечания
- Лимит длины ответа управляется `LLM_MAX_TOKENS` (по умолчанию 1200). Если ответ обрезан, бот делает до 2 автопродолжений небольшими порциями.
- Системный промпт размещён в `prompts/alyavseprospala_prompt.txt` и содержит правило завершать мысль кратко при ограничении токенов.
